#!/usr/bin/env python3
"""
Compare evaluation results between two GAN runs.

Usage:
    python scripts/compare_runs.py \\
        --baseline <path_to_baseline_run> \\
        --roi <path_to_roi_run> \\
        --output runs/comparison_results.csv
"""

import argparse
import json
import csv
from pathlib import Path
from typing import Dict, Tuple, Optional
import sys


def load_metrics(eval_dir: Path) -> Optional[Dict]:
    """Load metrics.json from an evaluation directory"""
    metrics_file = eval_dir / "metrics.json"
    
    if not metrics_file.exists():
        print(f"‚ùå Error: metrics.json not found in {eval_dir}")
        return None
    
    try:
        with open(metrics_file, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"‚ùå Error loading metrics from {eval_dir}: {e}")
        return None


def get_run_name(run_dir: Path) -> str:
    """Extract run name from directory path"""
    return run_dir.name


def get_dataset_variant(run_name: str) -> str:
    """Infer dataset variant from run name"""
    if "roi" in run_name.lower():
        return "ROI"
    elif "baseline" in run_name.lower():
        return "Baseline"
    else:
        return "Unknown"


def format_per_class_fidelity(metrics: Dict) -> str:
    """Format per-class fidelity as compact JSON string"""
    per_class = metrics.get("per_class_label_fidelity", {})
    if not per_class:
        return "{}"
    
    # Format as compact JSON
    return json.dumps(per_class, separators=(',', ':'))


def create_comparison_table(
    baseline_run: Path,
    roi_run: Path,
    output_csv: Path
) -> Tuple[list, bool]:
    """
    Create comparison CSV table with metrics from both runs.
    
    Returns:
        (rows, success_flag) where rows is list of dicts for CSV
    """
    rows = []
    success = True
    
    # Load baseline metrics
    print(f"\nüìÅ Loading baseline metrics from: {baseline_run}")
    baseline_eval_dir = baseline_run / "evaluation"
    baseline_metrics = load_metrics(baseline_eval_dir)
    
    if baseline_metrics is None:
        print(f"‚ùå Failed to load baseline metrics")
        success = False
    else:
        baseline_name = get_run_name(baseline_run)
        print(f"‚úì Loaded baseline: {baseline_name}")
        rows.append({
            'run_name': baseline_name,
            'dataset_variant': get_dataset_variant(baseline_name),
            'FID': f"{baseline_metrics.get('overall_fid', 'N/A'):.4f}" if isinstance(baseline_metrics.get('overall_fid'), (int, float)) else 'N/A',
            'label_fidelity': f"{baseline_metrics.get('label_fidelity', 'N/A'):.4f}" if isinstance(baseline_metrics.get('label_fidelity'), (int, float)) else 'N/A',
            'classifier_val_accuracy': f"{baseline_metrics.get('classifier_real_accuracy', 'N/A'):.4f}" if isinstance(baseline_metrics.get('classifier_real_accuracy'), (int, float)) else 'N/A',
            'per_class_fidelity_json': format_per_class_fidelity(baseline_metrics)
        })
    
    # Load ROI metrics
    print(f"\nüìÅ Loading ROI metrics from: {roi_run}")
    roi_eval_dir = roi_run / "evaluation"
    roi_metrics = load_metrics(roi_eval_dir)
    
    if roi_metrics is None:
        print(f"‚ùå Failed to load ROI metrics")
        success = False
    else:
        roi_name = get_run_name(roi_run)
        print(f"‚úì Loaded ROI: {roi_name}")
        rows.append({
            'run_name': roi_name,
            'dataset_variant': get_dataset_variant(roi_name),
            'FID': f"{roi_metrics.get('overall_fid', 'N/A'):.4f}" if isinstance(roi_metrics.get('overall_fid'), (int, float)) else 'N/A',
            'label_fidelity': f"{roi_metrics.get('label_fidelity', 'N/A'):.4f}" if isinstance(roi_metrics.get('label_fidelity'), (int, float)) else 'N/A',
            'classifier_val_accuracy': f"{roi_metrics.get('classifier_real_accuracy', 'N/A'):.4f}" if isinstance(roi_metrics.get('classifier_real_accuracy'), (int, float)) else 'N/A',
            'per_class_fidelity_json': format_per_class_fidelity(roi_metrics)
        })
    
    # Save CSV
    if rows:
        output_csv.parent.mkdir(parents=True, exist_ok=True)
        with open(output_csv, 'w', newline='') as f:
            fieldnames = ['run_name', 'dataset_variant', 'FID', 'label_fidelity', 'classifier_val_accuracy', 'per_class_fidelity_json']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)
        
        print(f"\n‚úì Saved comparison CSV: {output_csv}")
    
    return rows, success


def generate_markdown_report(
    baseline_run: Path,
    roi_run: Path,
    comparison_rows: list,
    output_md: Path
) -> bool:
    """Generate a markdown comparison report"""
    
    # Load detailed metrics for better analysis
    baseline_metrics = load_metrics(baseline_run / "evaluation")
    roi_metrics = load_metrics(roi_run / "evaluation")
    
    if baseline_metrics is None or roi_metrics is None:
        return False
    
    output_md.parent.mkdir(parents=True, exist_ok=True)
    
    # Extract metrics
    baseline_name = get_run_name(baseline_run)
    roi_name = get_run_name(roi_run)
    
    baseline_fid = baseline_metrics.get('overall_fid', 0)
    roi_fid = roi_metrics.get('overall_fid', 0)
    fid_diff = abs(roi_fid - baseline_fid)
    fid_pct = (fid_diff / baseline_fid * 100) if baseline_fid > 0 else 0
    
    baseline_fidelity = baseline_metrics.get('label_fidelity', 0)
    roi_fidelity = roi_metrics.get('label_fidelity', 0)
    fidelity_diff = abs(roi_fidelity - baseline_fidelity)
    fidelity_pct = (fidelity_diff / baseline_fidelity * 100) if baseline_fidelity > 0 else 0
    
    baseline_acc = baseline_metrics.get('classifier_real_accuracy', 0)
    roi_acc = roi_metrics.get('classifier_real_accuracy', 0)
    acc_diff = abs(roi_acc - baseline_acc)
    
    baseline_per_class = baseline_metrics.get('per_class_label_fidelity', {})
    roi_per_class = roi_metrics.get('per_class_label_fidelity', {})
    
    # Generate report
    report = f"""# GAN Evaluation Comparison Report

## Overview

This report compares the evaluation metrics between two conditional GAN runs on the NEU steel surface defects dataset:

- **Baseline Run**: `{baseline_name}`
- **ROI Run**: `{roi_name}`

---

## Summary Statistics

| Metric | Baseline | ROI | Difference |
|--------|----------|-----|-----------|
| **FID Score** | {baseline_fid:.4f} | {roi_fid:.4f} | {fid_diff:+.4f} ({fid_pct:+.1f}%) |
| **Label Fidelity** | {baseline_fidelity:.4f} | {roi_fidelity:.4f} | {fidelity_diff:+.4f} ({fidelity_pct:+.1f}%) |
| **Classifier Accuracy** | {baseline_acc:.4f} | {roi_acc:.4f} | {acc_diff:+.4f} |

---

## Detailed Analysis

### 1. FID Score (Feature Distribution Quality)

**Interpretation**: Lower FID indicates generated images have feature distributions closer to real images.

- **Baseline**: {baseline_fid:.4f}
- **ROI**: {roi_fid:.4f}
- **Trend**: {"‚úÖ ROI performs better" if roi_fid < baseline_fid else "‚ö†Ô∏è Baseline performs better"}

**Insights**:
"""
    
    if roi_fid < baseline_fid:
        report += f"- ROI images achieve {fid_pct:.1f}% lower FID (closer to real distribution)\n"
        report += "- ROI dataset preprocessing improves feature alignment\n"
    else:
        report += f"- Baseline achieves {fid_pct:.1f}% lower FID\n"
        report += "- Baseline 128√ó128 images have better preserved features\n"
    
    report += f"""
### 2. Label Fidelity (Class Conditioning Quality)

**Interpretation**: Higher label fidelity means more generated images are correctly classified as their conditioning label.

- **Baseline**: {baseline_fidelity:.4f} ({baseline_fidelity*100:.1f}%)
- **ROI**: {roi_fidelity:.4f} ({roi_fidelity*100:.1f}%)
- **Trend**: {"‚úÖ ROI performs better" if roi_fidelity > baseline_fidelity else "‚ö†Ô∏è Baseline performs better"}

**Insights**:
"""
    
    if roi_fidelity > baseline_fidelity:
        report += f"- ROI achieves {fidelity_pct:.1f}% higher fidelity\n"
        report += "- ROI cropping focuses on defect regions, improving class conditioning\n"
    else:
        report += f"- Baseline achieves {fidelity_pct:.1f}% higher fidelity\n"
        report += "- Full images provide more context for class generation\n"
    
    report += f"""
### 3. Classifier Accuracy (Real Data Baseline)

**Interpretation**: Classifier accuracy on real validation data indicates how reliable the label fidelity metric is.

- **Baseline**: {baseline_acc:.4f} ({baseline_acc*100:.1f}%)
- **ROI**: {roi_acc:.4f} ({roi_acc*100:.1f}%)

**Notes**:
- Both classifiers achieve >70% accuracy on real data
- Suggests fidelity measurements are reasonably reliable
- Small differences in classifier performance likely due to training randomness

---

## Per-Class Fidelity Analysis

### Baseline Results
"""
    
    for class_name, fidelity in sorted(baseline_per_class.items()):
        report += f"- **{class_name}**: {fidelity:.4f} ({fidelity*100:.1f}%)\n"
    
    report += "\n### ROI Results\n"
    
    for class_name, fidelity in sorted(roi_per_class.items()):
        report += f"- **{class_name}**: {fidelity:.4f} ({fidelity*100:.1f}%)\n"
    
    report += f"""
---

## Key Differences

### Dataset Characteristics

| Aspect | Baseline | ROI |
|--------|----------|-----|
| **Input Size** | 128√ó128 full images | 128√ó128 cropped defect regions |
| **Context** | Full steel surface | Defect-focused |
| **Feature Distribution** | Broader (more background) | Concentrated (less noise) |
| **Preprocessing** | Resizing only | ROI cropping + resizing |

### Generation Trade-offs

**Baseline Advantages**:
- More context from full images
- Better for diverse generation
- Captures environmental context

**ROI Advantages**:
- Focused on defect features
- Potentially better class distinction
- Less background variation

---

## Interpretation Guidance

### When to Use Baseline Results
- ‚úì When diversity of generation is important
- ‚úì When environmental context matters
- ‚úì For general-purpose defect generation

### When to Use ROI Results
- ‚úì When focusing on defect-specific features
- ‚úì When minimizing background noise is critical
- ‚úì For detailed defect analysis

---

## Limitations

### Dataset Size
- Each run generated only 30 images per class (180 total)
- Small sample size may affect FID stability
- Per-class fidelity can be noisy with low sample counts

### Classifier Dependency
- Label fidelity relies on a single classifier
- Classifier trained for 5 epochs (quick baseline)
- May not fully capture all class characteristics
- Could be improved with longer training / better architecture

### Evaluation Scope
- Evaluated on validation set only
- Single checkpoint per model
- Doesn't account for:
  - Training convergence
  - Temporal stability of generation
  - Mode collapse in specific classes

### Metric Limitations
- **FID**: Assumes Gaussian feature distributions (may not hold exactly)
- **Label Fidelity**: Depends on classifier quality (70-75% real accuracy)
- **Per-Class**: Small samples (30 images) create high variance

---

## Recommendations

1. **For Publication**:
   - Report both FID and label fidelity
   - Include per-class breakdowns
   - Discuss dataset variant selection rationale
   - Note the evaluation methodology in appendix

2. **For Further Analysis**:
   - Increase sample size (100+ per class) for stable FID
   - Train classifier for full convergence (10+ epochs)
   - Evaluate on multiple checkpoints for stability
   - Consider using multiple classifiers for fidelity

3. **Dataset Selection**:
   - Choose based on application requirements:
     - **Baseline**: If context/diversity matters
     - **ROI**: If focused defect features matter

---

## Files Generated

- `comparison_results.csv` - Structured comparison metrics (CSV format)
- `comparison_report.md` - This report (Markdown format)

---

*Generated for thesis evaluation of conditional GANs on NEU steel surface defects dataset.*
"""
    
    with open(output_md, 'w') as f:
        f.write(report)
    
    print(f"‚úì Saved markdown report: {output_md}")
    return True


def main():
    parser = argparse.ArgumentParser(
        description="Compare evaluation results between baseline and ROI GAN runs",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/compare_runs.py \\
    --baseline runs/cgan_baseline_128 \\
    --roi runs/cgan_roi_128 \\
    --output runs/cgan_comparison.csv

  python scripts/compare_runs.py \\
    --baseline runs/qcgan_baseline_128 \\
    --roi runs/qcgan_roi_128 \\
    --report runs/qcgan_comparison_report.md
        """
    )
    
    parser.add_argument(
        '--baseline',
        type=str,
        required=True,
        help='Path to baseline run directory (e.g., runs/cgan_baseline_128)'
    )
    parser.add_argument(
        '--roi',
        type=str,
        required=True,
        help='Path to ROI run directory (e.g., runs/cgan_roi_128)'
    )
    parser.add_argument(
        '--output',
        type=str,
        default='runs/comparison_results.csv',
        help='Output CSV file path (default: runs/comparison_results.csv)'
    )
    parser.add_argument(
        '--report',
        type=str,
        default='runs/comparison_report.md',
        help='Output markdown report path (default: runs/comparison_report.md)'
    )
    
    args = parser.parse_args()
    
    baseline_run = Path(args.baseline)
    roi_run = Path(args.roi)
    output_csv = Path(args.output)
    output_md = Path(args.report)
    
    # Validate inputs
    print("=" * 70)
    print("üîç GAN EVALUATION COMPARISON")
    print("=" * 70)
    
    if not baseline_run.exists():
        print(f"‚ùå Error: Baseline run directory not found: {baseline_run}")
        sys.exit(1)
    
    if not roi_run.exists():
        print(f"‚ùå Error: ROI run directory not found: {roi_run}")
        sys.exit(1)
    
    # Create comparison table
    print("\n" + "=" * 70)
    print("üìä CREATING COMPARISON TABLE")
    print("=" * 70)
    
    rows, table_success = create_comparison_table(baseline_run, roi_run, output_csv)
    
    if not table_success:
        print("\n‚ö†Ô∏è  Warning: Could not load all metrics. Continuing with partial data...")
    
    # Display table
    print("\n" + "=" * 70)
    print("üìã COMPARISON RESULTS")
    print("=" * 70 + "\n")
    
    if rows:
        # Print header
        print(f"{'Run Name':<30} {'Dataset':<12} {'FID':<10} {'Fidelity':<12} {'Accuracy':<10}")
        print("-" * 75)
        
        # Print rows
        for row in rows:
            print(f"{row['run_name']:<30} {row['dataset_variant']:<12} {row['FID']:<10} {row['label_fidelity']:<12} {row['classifier_val_accuracy']:<10}")
        
        print()
    
    # Generate markdown report
    print("=" * 70)
    print("üìù GENERATING MARKDOWN REPORT")
    print("=" * 70)
    
    report_success = generate_markdown_report(baseline_run, roi_run, rows, output_md)
    
    if not report_success:
        print("\n‚ö†Ô∏è  Warning: Could not generate full markdown report")
    
    # Summary
    print("\n" + "=" * 70)
    print("‚úÖ COMPARISON COMPLETE")
    print("=" * 70)
    print(f"\nüìÅ Output Files:")
    print(f"  ‚Ä¢ CSV:      {output_csv}")
    print(f"  ‚Ä¢ Report:   {output_md}")
    print()


if __name__ == '__main__':
    main()
